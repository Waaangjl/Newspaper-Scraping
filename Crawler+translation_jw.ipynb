{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_to_page(page_num):\n",
    "    # Wait for the page box to become visible\n",
    "    page_box = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"pagebox\")))\n",
    "    \n",
    "    i = 1\n",
    "    while True:\n",
    "        try:\n",
    "            # Find and click on the page link\n",
    "            page = page_box.find_element(By.LINK_TEXT, str(page_num))\n",
    "            page.click()\n",
    "            time.sleep(1)\n",
    "            break\n",
    "        except:\n",
    "            i += 4\n",
    "            # Find and click on the page link\n",
    "            page = page_box.find_element(By.LINK_TEXT, str(i))\n",
    "            page.click()\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://navi.cnki.net/knavi/newspapers/HQSB/detail?uniplatform=NZKPT\")\n",
    "\n",
    "search_box = WebDriverWait(driver, 10).until(\n",
    "    EC.visibility_of_element_located((By.ID, \"J_searchTxt\"))\n",
    ")\n",
    "\n",
    "# 在输入框中输入文字\n",
    "search_box.send_keys(\"疫情\")\n",
    "time.sleep(2)\n",
    "\n",
    "# press enter\n",
    "search_box.send_keys(Keys.ENTER)\n",
    "time.sleep(1)\n",
    "sort_dropdown = driver.find_element(By.CLASS_NAME, \"sort_select_default\")\n",
    "\n",
    "# hover over the sorting options dropdown\n",
    "ActionChains(driver).move_to_element(sort_dropdown).perform()\n",
    "\n",
    "# click and then select the date option\n",
    "date_option = WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.XPATH, '//a[@name=\"DT\"]'))\n",
    ")\n",
    "date_option.click()\n",
    "time.sleep(1)\n",
    "\n",
    "go_to_page(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the <tbody> element within the table (assuming there is only one table)\n",
    "tbody = driver.find_element(By.XPATH, \"//table/tbody\")\n",
    "\n",
    "# Find all <tr> elements within the <tbody>\n",
    "rows = tbody.find_elements(By.TAG_NAME, \"tr\")\n",
    "for row in rows:\n",
    "    link = row.find_element(By.XPATH, \".//td[@class='name']/a\").get_attribute(\"href\")\n",
    "    # click on the link\n",
    "    driver.get(link)\n",
    "    html_link = driver.find_element(By.CLASS_NAME, \"btn-html\")\n",
    "    html_link.click()\n",
    "    time.sleep(1.1)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please log in through WeChat, and then run the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to the new tab\n",
    "driver.switch_to.window(driver.window_handles[1])\n",
    "driver.close()\n",
    "driver.switch_to.window(driver.window_handles[0])\n",
    "driver.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the page\n",
    "def download_page(page_num):\n",
    "    go_to_page(page_num)\n",
    "    \n",
    "    # Find the <tbody> element within the table (assuming there is only one table)\n",
    "    tbody = driver.find_element(By.XPATH, \"//table/tbody\")\n",
    "    # Find all <tr> elements within the <tbody>\n",
    "    rows = tbody.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "    links = []\n",
    "    contents = []\n",
    "    keywords = []\n",
    "    for row in rows:\n",
    "        link = row.find_element(By.XPATH, \".//td[@class='name']/a\").get_attribute(\"href\")\n",
    "        links.append(link)\n",
    "        driver.get(link)\n",
    "        keyword = driver.find_element(By.CLASS_NAME, \"keywords\").text\n",
    "        keywords.append(keyword)\n",
    "        time.sleep(1.2)\n",
    "        html_link = driver.find_element(By.CLASS_NAME, \"btn-html\")\n",
    "        html_link.click()\n",
    "        time.sleep(1.5)\n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "        time.sleep(0.5)\n",
    "        elements = driver.find_elements(By.XPATH, \".//*[contains(@class, 'p1') or contains(@class, 'anchor-tag')]\")\n",
    "        text = \"\"\n",
    "        for element in elements:\n",
    "            text += element.text\n",
    "            text += \"\\n\"\n",
    "        contents.append(text)\n",
    "        driver.close()\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        driver.back()\n",
    "        time.sleep(1)\n",
    "\n",
    "    tbody_text = tbody.text\n",
    "    tbody_text = tbody_text.split(\"\\n\")\n",
    "\n",
    "    for i in range(len(tbody_text)):\n",
    "        if i % 2 == 0:\n",
    "            # split by the first space\n",
    "            tbody_text[i] = tbody_text[i].split(\" \", 1)\n",
    "        else:\n",
    "            if \";\" not in tbody_text[i]:\n",
    "                for k in range(len(tbody_text[i])):\n",
    "                    if tbody_text[i][k].isnumeric():\n",
    "                        tbody_text[i] = tbody_text[i][k:]\n",
    "                        tbody_text[i] = 'NA; ' + tbody_text[i]\n",
    "                        break\n",
    "            tbody_text[i] = tbody_text[i].split(\" \")\n",
    "            if len(tbody_text[i]) == 3:\n",
    "                tbody_text[i].append('0')\n",
    "\n",
    "    # combine the odd and even rows to a large list\n",
    "    tbody_text = [item for sublist in tbody_text for item in sublist]\n",
    "    tbody_text = np.array(tbody_text).reshape(-1, 6)\n",
    "    tbody_text = pd.DataFrame(tbody_text, columns=[\"Number\", \"Title\", \"Author\", \"banHao\", \"Date\", \"Download\"])\n",
    "    tbody_text[\"Links\"] = links\n",
    "    tbody_text[\"Contents\"] = contents\n",
    "    tbody_text[\"Keywords\"] = keywords\n",
    "    page_df = tbody_text.copy()\n",
    "    return page_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下面的代码可以根据自己需求调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 124 into shape (6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/karan/Documents/GitHub/local_central-level-propaganda/Crawler+translation_jw.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/karan/Documents/GitHub/local_central-level-propaganda/Crawler%2Btranslation_jw.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m end \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/karan/Documents/GitHub/local_central-level-propaganda/Crawler%2Btranslation_jw.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m (start, end \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/karan/Documents/GitHub/local_central-level-propaganda/Crawler%2Btranslation_jw.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     df_current \u001b[39m=\u001b[39m download_page(i)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/karan/Documents/GitHub/local_central-level-propaganda/Crawler%2Btranslation_jw.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     df_current\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mhq\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8-sig\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/karan/Documents/GitHub/local_central-level-propaganda/Crawler%2Btranslation_jw.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m start:\n",
      "\u001b[1;32m/Users/karan/Documents/GitHub/local_central-level-propaganda/Crawler+translation_jw.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karan/Documents/GitHub/local_central-level-propaganda/Crawler%2Btranslation_jw.ipynb#X11sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# combine the odd and even rows to a large list\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karan/Documents/GitHub/local_central-level-propaganda/Crawler%2Btranslation_jw.ipynb#X11sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m tbody_text \u001b[39m=\u001b[39m [item \u001b[39mfor\u001b[39;00m sublist \u001b[39min\u001b[39;00m tbody_text \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m sublist]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/karan/Documents/GitHub/local_central-level-propaganda/Crawler%2Btranslation_jw.ipynb#X11sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m tbody_text \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(tbody_text)\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m6\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karan/Documents/GitHub/local_central-level-propaganda/Crawler%2Btranslation_jw.ipynb#X11sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m tbody_text \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(tbody_text, columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mNumber\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTitle\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mAuthor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mbanHao\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDownload\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karan/Documents/GitHub/local_central-level-propaganda/Crawler%2Btranslation_jw.ipynb#X11sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m tbody_text[\u001b[39m\"\u001b[39m\u001b[39mLinks\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m links\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 124 into shape (6)"
     ]
    }
   ],
   "source": [
    "start = 6\n",
    "end = 10\n",
    "for i in range (start, end + 1):\n",
    "    df_current = download_page(i)\n",
    "    df_current.to_csv(\"hq\" + str(i) + \".csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    if i == start:\n",
    "            df = df_current.copy()\n",
    "    else:\n",
    "        df = pd.concat([df, df_current], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据日期筛选\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df = df.sort_values(by=\"Date\")\n",
    "df = df[(df[\"Date\"] >= \"2022-04-01\") & (df[\"Date\"] <= \"2022-06-01\")]\n",
    "df = df.reset_index(drop=True)\n",
    "df.to_csv(\"data.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the directory where the CSV files are located\n",
    "directory = '/Users/karan/Desktop/昆山杜克/SW/CNKI_Crawler-main'  # Replace with the path to your directory containing the CSV files\n",
    "\n",
    "# Initialize an empty list to hold the DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop over the range of file names\n",
    "start = 15\n",
    "end = 31\n",
    "for i in range(start, end + 1):\n",
    "    file_name = f\"PD{i}.csv\"\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if os.path.exists(file_path):\n",
    "        # Read the CSV file and append it to the list of DataFrames\n",
    "        df_current = pd.read_csv(file_path, encoding=\"utf-8-sig\")\n",
    "        dfs.append(df_current)\n",
    "    else:\n",
    "        print(f\"{file_name} does not exist.\")\n",
    "\n",
    "# Concatenate all DataFrames in the list\n",
    "df_merged = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Write the merged DataFrame to a new CSV file\n",
    "output_file_path = os.path.join(directory, '人民日报.csv')\n",
    "df_merged.to_csv(output_file_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"Merged CSV file has been saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the merged CSV file\n",
    "merged_file_path = '/Users/karan/Desktop/昆山杜克/SW/CNKI_Crawler-main/人民日报.csv'  # Replace with the path to your merged CSV file\n",
    "\n",
    "# Define the path to save the filtered CSV file\n",
    "output_file_path = '/Users/karan/Desktop/昆山杜克/SW/CNKI_Crawler-main/人民日报.csv'  # Replace with the path to save the filtered CSV file\n",
    "\n",
    "# Define the date range\n",
    "start_date = '2022-04-01'  # YYYY-MM-DD format\n",
    "end_date = '2022-06-01'  # YYYY-MM-DD format\n",
    "\n",
    "# Check if the merged file exists\n",
    "if os.path.exists(merged_file_path):\n",
    "    # Read the merged CSV file\n",
    "    df_merged = pd.read_csv(merged_file_path, encoding=\"utf-8-sig\")\n",
    "    \n",
    "    # Convert the 'date' column to datetime\n",
    "    df_merged['Date'] = pd.to_datetime(df_merged['Date'], errors='coerce')\n",
    "    \n",
    "    # Filter the data based on the date range\n",
    "    df_filtered = df_merged[(df_merged['Date'] >= start_date) & (df_merged['Date'] <= end_date)]\n",
    "    \n",
    "    # Write the filtered DataFrame to a new CSV file\n",
    "    df_filtered.to_csv(output_file_path, index=False, encoding=\"utf-8-sig\")\n",
    "    \n",
    "    print(f\"Filtered CSV file has been saved to {output_file_path}\")\n",
    "else:\n",
    "    print(f\"{merged_file_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/CNKI/lib/python3.11/site-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Translating: 100%|██████████| 20/20 [16:08<00:00, 48.41s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import textwrap\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def translate_long_text(text, max_length=512):\n",
    "    \"\"\"Translate text that might be longer than the model's maximum length.\"\"\"\n",
    "    chunks = textwrap.wrap(text, width=max_length, break_long_words=False, replace_whitespace=False)\n",
    "    translated_chunks = [translate(chunk) for chunk in chunks]\n",
    "    return \" \".join(translated_chunks)\n",
    "\n",
    "def translate(text):\n",
    "    \"\"\"Translate Chinese text to English using the pre-trained model.\"\"\"\n",
    "    tokenized_text = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    translated_tokens = model.generate(tokenized_text)\n",
    "    translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Load your dataset\n",
    "input_path = '/Users/karan/Desktop/昆山杜克/SW/CNKI_Crawler-main/df15.csv'\n",
    "output_path = '/Users/karan/Desktop/昆山杜克/SW/CNKI_Crawler-main/df15.csv'\n",
    "\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Translate the 'content' column with progress visualization\n",
    "df['content_en'] = [translate_long_text(text) for text in tqdm(df['Contents'], desc=\"Translating\")]\n",
    "\n",
    "# Save the updated dataframe\n",
    "df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/CNKI/lib/python3.11/site-packages (4.66.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "晨 0\n",
      "九  \n",
      "123 许立 ;昭东 ;张静 ;莽九晨; 001 2022-05-14 6\n",
      "15\n",
      "  九\n",
      "静 莽\n",
      "123 许立 ;昭东 ;张静;莽九晨; 001 2022-05-14 6\n",
      "11\n",
      "  静\n",
      "东 张\n",
      "123 许立 ;昭东;张静;莽九晨; 001 2022-05-14 6\n",
      "7\n",
      "  东\n",
      "立 昭\n",
      "123 许立;昭东;张静;莽九晨; 001 2022-05-14 6\n"
     ]
    }
   ],
   "source": [
    "def is_chinese_char(char):\n",
    "    \"\"\"Check if the character is a Chinese character.\"\"\"\n",
    "    return '\\u4e00' <= char <= '\\u9fff'\n",
    "tbody_text = ['']\n",
    "i = 0\n",
    "tbody_text[i] = '123 许立 ;昭东 ;张静 ;莽九晨; 001 2022-05-14 6'\n",
    "semicolon_indices = [index for index, char in enumerate(tbody_text[i]) if char == \";\"]\n",
    "for k in range(len(semicolon_indices)):\n",
    "    semicolon_index = semicolon_indices[len(semicolon_indices) - k - 1]\n",
    "    print(semicolon_index)\n",
    "    print(tbody_text[i][semicolon_index - 1], tbody_text[i][semicolon_index + 2])\n",
    "    print(tbody_text[i][semicolon_index - 2], tbody_text[i][semicolon_index + 1])\n",
    "    # Check the character before the semicolon\n",
    "    if semicolon_index > 0 and is_chinese_char(tbody_text[i][semicolon_index - 1]) and is_chinese_char(tbody_text[i][semicolon_index + 2]):\n",
    "        # If there's a space after the semicolon, remove it\n",
    "        if tbody_text[i][semicolon_index + 1] == \" \":\n",
    "            tbody_text[i] = tbody_text[i][:semicolon_index + 1] + tbody_text[i][semicolon_index + 2:]\n",
    "    \n",
    "    # Check the character after the semicolon\n",
    "    if is_chinese_char(tbody_text[i][semicolon_index + 1]) and is_chinese_char(tbody_text[i][semicolon_index - 2]):\n",
    "        # If there's a space before the semicolon, remove it\n",
    "        if semicolon_index > 0 and tbody_text[i][semicolon_index - 1] == \" \":\n",
    "            tbody_text[i] = tbody_text[i][:semicolon_index - 1] + tbody_text[i][semicolon_index:]\n",
    "    print(tbody_text[i])\n",
    "\n",
    "tbody_text[i] = tbody_text[i].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
