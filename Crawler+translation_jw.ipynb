{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_to_page(page_num):\n",
    "    # Wait for the page box to become visible\n",
    "    page_box = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"pagebox\")))\n",
    "    \n",
    "    i = 1\n",
    "    while True:\n",
    "        try:\n",
    "            # Find and click on the page link\n",
    "            page = page_box.find_element(By.LINK_TEXT, str(page_num))\n",
    "            page.click()\n",
    "            time.sleep(1)\n",
    "            break\n",
    "        except:\n",
    "            i += 4\n",
    "            # Find and click on the page link\n",
    "            page = page_box.find_element(By.LINK_TEXT, str(i))\n",
    "            page.click()\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://navi.cnki.net/knavi/newspapers/RMRB/detail?uniplatform=NZKPT\")\n",
    "\n",
    "search_box = WebDriverWait(driver, 10).until(\n",
    "    EC.visibility_of_element_located((By.ID, \"J_searchTxt\"))\n",
    ")\n",
    "\n",
    "# 在输入框中输入文字\n",
    "search_box.send_keys(\"疫情\")\n",
    "time.sleep(2)\n",
    "\n",
    "# press enter\n",
    "search_box.send_keys(Keys.ENTER)\n",
    "time.sleep(1)\n",
    "sort_dropdown = driver.find_element(By.CLASS_NAME, \"sort_select_default\")\n",
    "\n",
    "# hover over the sorting options dropdown\n",
    "ActionChains(driver).move_to_element(sort_dropdown).perform()\n",
    "\n",
    "# click and then select the date option\n",
    "date_option = WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.XPATH, '//a[@name=\"DT\"]'))\n",
    ")\n",
    "date_option.click()\n",
    "time.sleep(1)\n",
    "\n",
    "go_to_page(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the <tbody> element within the table (assuming there is only one table)\n",
    "tbody = driver.find_element(By.XPATH, \"//table/tbody\")\n",
    "\n",
    "# Find all <tr> elements within the <tbody>\n",
    "rows = tbody.find_elements(By.TAG_NAME, \"tr\")\n",
    "for row in rows:\n",
    "    link = row.find_element(By.XPATH, \".//td[@class='name']/a\").get_attribute(\"href\")\n",
    "    # click on the link\n",
    "    driver.get(link)\n",
    "    html_link = driver.find_element(By.CLASS_NAME, \"btn-html\")\n",
    "    html_link.click()\n",
    "    time.sleep(1.1)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please log in through WeChat, and then run the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to the new tab\n",
    "driver.switch_to.window(driver.window_handles[1])\n",
    "driver.close()\n",
    "driver.switch_to.window(driver.window_handles[0])\n",
    "driver.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the page\n",
    "def download_page(page_num):\n",
    "    go_to_page(page_num)\n",
    "    \n",
    "    # Find the <tbody> element within the table (assuming there is only one table)\n",
    "    tbody = driver.find_element(By.XPATH, \"//table/tbody\")\n",
    "    # Find all <tr> elements within the <tbody>\n",
    "    rows = tbody.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "    links = []\n",
    "    contents = []\n",
    "    keywords = []\n",
    "    for row in rows:\n",
    "        link = row.find_element(By.XPATH, \".//td[@class='name']/a\").get_attribute(\"href\")\n",
    "        links.append(link)\n",
    "        driver.get(link)\n",
    "        keyword = driver.find_element(By.CLASS_NAME, \"keywords\").text\n",
    "        keywords.append(keyword)\n",
    "        time.sleep(1.2)\n",
    "        html_link = driver.find_element(By.CLASS_NAME, \"btn-html\")\n",
    "        html_link.click()\n",
    "        time.sleep(1.5)\n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "        time.sleep(0.5)\n",
    "        elements = driver.find_elements(By.XPATH, \".//*[contains(@class, 'p1') or contains(@class, 'anchor-tag')]\")\n",
    "        text = \"\"\n",
    "        for element in elements:\n",
    "            text += element.text\n",
    "            text += \"\\n\"\n",
    "        contents.append(text)\n",
    "        driver.close()\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        driver.back()\n",
    "        time.sleep(1)\n",
    "\n",
    "    tbody_text = tbody.text\n",
    "    tbody_text = tbody_text.split(\"\\n\")\n",
    "\n",
    "    for i in range(len(tbody_text)):\n",
    "        if i % 2 == 0:\n",
    "            # split by the first space\n",
    "            tbody_text[i] = tbody_text[i].split(\" \", 1)\n",
    "        else:\n",
    "            if \";\" not in tbody_text[i]:\n",
    "                for k in range(len(tbody_text[i])):\n",
    "                    if tbody_text[i][k].isnumeric():\n",
    "                        tbody_text[i] = tbody_text[i][k:]\n",
    "                        tbody_text[i] = 'NA; ' + tbody_text[i]\n",
    "                        break\n",
    "            tbody_text[i] = tbody_text[i].split(\" \")\n",
    "            if len(tbody_text[i]) == 3:\n",
    "                tbody_text[i].append('0')\n",
    "\n",
    "    # combine the odd and even rows to a large list\n",
    "    tbody_text = [item for sublist in tbody_text for item in sublist]\n",
    "    tbody_text = np.array(tbody_text).reshape(-1, 6)\n",
    "    tbody_text = pd.DataFrame(tbody_text, columns=[\"Number\", \"Title\", \"Author\", \"banHao\", \"Date\", \"Download\"])\n",
    "    tbody_text[\"Links\"] = links\n",
    "    tbody_text[\"Contents\"] = contents\n",
    "    tbody_text[\"Keywords\"] = keywords\n",
    "    page_df = tbody_text.copy()\n",
    "    return page_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下面的代码可以根据自己需求调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 21\n",
    "end = 31\n",
    "for i in range (start, end + 1):\n",
    "    df_current = download_page(i)\n",
    "    df_current.to_csv(\"PD\" + str(i) + \".csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    if i == start:\n",
    "            df = df_current.copy()\n",
    "    else:\n",
    "        df = pd.concat([df, df_current], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据日期筛选\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df = df.sort_values(by=\"Date\")\n",
    "df = df[(df[\"Date\"] >= \"2022-04-01\") & (df[\"Date\"] <= \"2022-06-01\")]\n",
    "df = df.reset_index(drop=True)\n",
    "df.to_csv(\"data.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the directory where the CSV files are located\n",
    "directory = '/Users/karan/Desktop/昆山杜克/SW/CNKI_Crawler-main'  # Replace with the path to your directory containing the CSV files\n",
    "\n",
    "# Initialize an empty list to hold the DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop over the range of file names\n",
    "start = 15\n",
    "end = 31\n",
    "for i in range(start, end + 1):\n",
    "    file_name = f\"PD{i}.csv\"\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if os.path.exists(file_path):\n",
    "        # Read the CSV file and append it to the list of DataFrames\n",
    "        df_current = pd.read_csv(file_path, encoding=\"utf-8-sig\")\n",
    "        dfs.append(df_current)\n",
    "    else:\n",
    "        print(f\"{file_name} does not exist.\")\n",
    "\n",
    "# Concatenate all DataFrames in the list\n",
    "df_merged = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Write the merged DataFrame to a new CSV file\n",
    "output_file_path = os.path.join(directory, '人民日报.csv')\n",
    "df_merged.to_csv(output_file_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"Merged CSV file has been saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the merged CSV file\n",
    "merged_file_path = '/Users/karan/Desktop/昆山杜克/SW/CNKI_Crawler-main/人民日报.csv'  # Replace with the path to your merged CSV file\n",
    "\n",
    "# Define the path to save the filtered CSV file\n",
    "output_file_path = '/Users/karan/Desktop/昆山杜克/SW/CNKI_Crawler-main/人民日报.csv'  # Replace with the path to save the filtered CSV file\n",
    "\n",
    "# Define the date range\n",
    "start_date = '2022-04-01'  # YYYY-MM-DD format\n",
    "end_date = '2022-06-01'  # YYYY-MM-DD format\n",
    "\n",
    "# Check if the merged file exists\n",
    "if os.path.exists(merged_file_path):\n",
    "    # Read the merged CSV file\n",
    "    df_merged = pd.read_csv(merged_file_path, encoding=\"utf-8-sig\")\n",
    "    \n",
    "    # Convert the 'date' column to datetime\n",
    "    df_merged['Date'] = pd.to_datetime(df_merged['Date'], errors='coerce')\n",
    "    \n",
    "    # Filter the data based on the date range\n",
    "    df_filtered = df_merged[(df_merged['Date'] >= start_date) & (df_merged['Date'] <= end_date)]\n",
    "    \n",
    "    # Write the filtered DataFrame to a new CSV file\n",
    "    df_filtered.to_csv(output_file_path, index=False, encoding=\"utf-8-sig\")\n",
    "    \n",
    "    print(f\"Filtered CSV file has been saved to {output_file_path}\")\n",
    "else:\n",
    "    print(f\"{merged_file_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/CNKI/lib/python3.11/site-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Translating: 100%|██████████| 20/20 [16:08<00:00, 48.41s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import textwrap\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def translate_long_text(text, max_length=512):\n",
    "    \"\"\"Translate text that might be longer than the model's maximum length.\"\"\"\n",
    "    chunks = textwrap.wrap(text, width=max_length, break_long_words=False, replace_whitespace=False)\n",
    "    translated_chunks = [translate(chunk) for chunk in chunks]\n",
    "    return \" \".join(translated_chunks)\n",
    "\n",
    "def translate(text):\n",
    "    \"\"\"Translate Chinese text to English using the pre-trained model.\"\"\"\n",
    "    tokenized_text = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    translated_tokens = model.generate(tokenized_text)\n",
    "    translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Load your dataset\n",
    "input_path = '/Users/karan/Desktop/昆山杜克/SW/CNKI_Crawler-main/df15.csv'\n",
    "output_path = '/Users/karan/Desktop/昆山杜克/SW/CNKI_Crawler-main/df15.csv'\n",
    "\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Translate the 'content' column with progress visualization\n",
    "df['content_en'] = [translate_long_text(text) for text in tqdm(df['Contents'], desc=\"Translating\")]\n",
    "\n",
    "# Save the updated dataframe\n",
    "df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/CNKI/lib/python3.11/site-packages (4.66.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
